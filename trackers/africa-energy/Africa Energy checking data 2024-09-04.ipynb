{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ee1a1b5f-1e68-493d-816c-12ece9c80390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import time\n",
    "from helper_functions import *\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93a541b",
   "metadata": {},
   "source": [
    "Questions for Maisie:\n",
    "\n",
    "when can i rename cols\n",
    "how check that your manipulations haven't lost data\n",
    "what should be printed\n",
    "what is ideal way to log errors - xls or print out \n",
    "how many files to use how to organize them\n",
    "how organize the folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a135098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # one off source file update # TODO\n",
    "# key = '1WRRFRuR9mWxZpko-VMYH5xm0LzRX_qM7W73nGGi4Jmg'\n",
    "# tab = 'Data'\n",
    "# tracker='bio' \n",
    "# # maybe download and add to pre test, if pass then upload to data team folder and the source folder, use to create the maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a5ced522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# little configurations\n",
    "testing_source = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302287f1-7178-426a-b2de-aff14c56f27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: CONFIGURATIONS TO BE CHANGED AS NEEDED PER TESTER AND CYCLE\n",
    "\n",
    "\n",
    "\n",
    "testing_source_path = '/Users/gem-tah/Desktop/GEM_INFO/GEM_WORK/earthrise-maps/testing/source/'\n",
    "testing_final_path = '/Users/gem-tah/Desktop/GEM_INFO/GEM_WORK/earthrise-maps/testing/final/'\n",
    "\n",
    "# use the same version of each tracker as was used for current version of Africa Energy map\n",
    "coal_mines_file = 'Global-Coal-Mine-Tracker-April-2024 DATA TEAM COPY.xlsx' #'Global-Coal-Mine-Tracker-April-2024.xlsx'\n",
    "gogpt_file = 'Global Oil and Gas Plant Tracker (GOGPT) - August 2024 DRAFT v2 DATA TEAM COPY.xlsx' #'Global Oil and Gas Plant Tracker (GOGPT) - February 2024 v4.xlsx'\n",
    "gcpt_file = 'Global-Coal-Plant-Tracker-July-2024 DATA TEAM COPY.xlsx' #'Global-Coal-Plant-Tracker-July-2024.xlsx'\n",
    "hydro_file = 'Global-Hydropower-Tracker-April-2024 DATA TEAM COPY.xlsx' #'Global-Hydropower-Tracker-April-2024.xlsx'\n",
    "bio_file = 'Global-Bioenergy-Power-Tracker-GBPT-V1 DATA TEAM COPY.xlsx' #'Global-Bioenergy-Power-Tracker-2023-11.xlsx'\n",
    "wind_file = 'Global-Wind-Power-Tracker-June-2024 DATA TEAM COPY.xlsx' #'Global-Wind-Power-Tracker-June-2024.xlsx'\n",
    "\n",
    "goget_file = 'Global Oil and Gas Extraction Tracker - 2024-03-08_1205 DATA TEAM COPY.xlsx'\n",
    "goget_formatted ='GOGET Earthgenome file 2024-07-03_200828.xlsx'\n",
    "ggit_file = 'GEM-GGIT-Gas-Pipelines-2023-12 copy.geojson'\n",
    "ggit_lng_file = 'GEM-GGIT-LNG-Terminals-2024-01 copy.geojson'\n",
    "nuclear_file = 'Global-Nuclear-Power-Tracker-July-2024 DATA TEAM COPY.xlsx'\n",
    "coal_terminals_file = 'Global-Coal-Terminals-Tracker-December-2023 DATA TEAM COPY.xlsx'\n",
    "goit_file = 'GEM-GOIT-Oil-NGL-Pipelines-2024-06 copy.geojson'\n",
    "geothermal_file = 'Global-Geothermal-Power-Tracker-July-2023 DATA TEAM COPY.xlsx'\n",
    "solar_file = 'Global-Solar-Power-Tracker-June-2024 DATA TEAM COPY.xlsx'\n",
    "coal_term_file = \"Global-Coal-Terminals-Tracker-December-2023 DATA TEAM COPY.xlsx\"\n",
    "\n",
    "# create all source dfs\n",
    "dfs = []\n",
    "for sheet_name  in ['Main data', 'Production & reserves']: # TODO add in Scotts code, main data is main data, pull in prod as needed or use formatted for immediate test\n",
    "    df = pd.read_excel(testing_source_path + goget_file, sheet_name=sheet_name,\n",
    "        dtype=dtype_spec)\n",
    "    dfs += [df]\n",
    "goget = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "goget_formatted = pd.read_excel(testing_source_path + goget_formatted, sheet_name='Data')\n",
    "ggit_lng = gpd.read_file(testing_source_path + ggit_lng_file)\n",
    "ggit = gpd.read_file(testing_source_path + ggit_file)\n",
    "goit = gpd.read_file(testing_source_path + goit_file)\n",
    "nuclear = pd.read_excel(testing_source_path + nuclear_file, sheet_name='Data',\n",
    "        dtype=dtype_spec)\n",
    "coal_term = pd.read_excel(testing_source_path + coal_terminals_file, sheet_name='Coal Terminals',\n",
    "        dtype=dtype_spec)\n",
    "geo = pd.read_excel(testing_source_path + geothermal_file, sheet_name= 'Data',\n",
    "        dtype=dtype_spec)\n",
    "dfs = []\n",
    "for sheet_name in['20 MW+', '1-20 MW']:\n",
    "    df = pd.read_excel(testing_source_path + solar_file, sheet_name=sheet_name,\n",
    "        dtype=dtype_spec)\n",
    "    dfs += [df]\n",
    "solar = pd.concat(dfs).reset_index(drop=True) \n",
    "gogpt = pd.read_excel(testing_source_path + gogpt_file, sheet_name = 'Gas & Oil units',\n",
    "        dtype=dtype_spec)\n",
    "gcpt = pd.read_excel(\n",
    "    testing_source_path + gcpt_file,\n",
    "    sheet_name = 'Units',\n",
    "        dtype=dtype_spec\n",
    ")\n",
    "dfs = [] # initialize\n",
    "for sheet_name in ['Global Coal Mine Tracker (Non-C', 'Global Coal Mine Tracker (Close']:\n",
    "    df = pd.read_excel(\n",
    "        testing_source_path +\n",
    "        coal_mines_file,\n",
    "        sheet_name = sheet_name,\n",
    "        dtype=dtype_spec\n",
    "        \n",
    "    )\n",
    "    if sheet_name == 'Global Coal Mine Tracker (Close':\n",
    "        if 'Status' not in df.columns.to_list():\n",
    "            df['Status'] = 'Retired'\n",
    "    dfs += [df]\n",
    "coal_mines = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "hydro = pd.read_excel(\n",
    "    testing_source_path + hydro_file,\n",
    "    sheet_name = 'Data',\n",
    "        dtype=dtype_spec\n",
    ")\n",
    "\n",
    "dfs= []\n",
    "for sheet_name in ['Data', 'Below Threshold']:\n",
    "    df = pd.read_excel(\n",
    "    testing_source_path + wind_file,\n",
    "    sheet_name = sheet_name,\n",
    "    dtype=dtype_spec\n",
    ")\n",
    "    dfs += [df]\n",
    "wind = pd.concat(dfs).reset_index(drop=True) \n",
    "\n",
    "\n",
    "bio = pd.read_excel(\n",
    "    testing_source_path + bio_file,\n",
    "    sheet_name = 'Data',\n",
    "        dtype=dtype_spec\n",
    ")\n",
    "\n",
    "coal_terminals = pd.read_excel(testing_source_path + coal_term_file,\n",
    "                               sheet_name = 'Coal Terminals',\n",
    "                               dtype=dtype_spec)\n",
    "\n",
    "map_to_test = 'latam' \n",
    "\n",
    "for file in os.listdir(testing_final_path): \n",
    "    if (map_to_test in file) & file.endswith(\".geojson\"):\n",
    "        final_map = gpd.read_file(testing_final_path + file)\n",
    "    elif (map_to_test in file) & file.endswith(\".xlsx\"):\n",
    "        # final_data = pd.ExcelFile(testing_final_path + file)\n",
    "        # Read all sheets into a dictionary\n",
    "        final_data_dict = pd.read_excel(testing_final_path + file, sheet_name=None) # ERROR ENGINE CANNOT BE DETERMINED\n",
    "    # TODO add in the summary data files when we generate them\n",
    "print(f\"We will be testing for map {map_to_test}.\")\n",
    "print(f\"Make sure all local source files have been updated for this cycle.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcab9d5",
   "metadata": {},
   "source": [
    "TEST SOURCE FILE FOR MISSING OR MISFORMATTED COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2710cfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing_source == True:\n",
    "\n",
    "\n",
    "    misformatted_cols_check = ['geometry', 'Latitude', 'Longitude'] #'status', 'url', 'areas',\n",
    "    misformatted = {}\n",
    "    list_of_trackers = [(coal_terminals, 'GCTT'), (coal_mines, 'GCMT'), (gcpt,'GCPT'), (gogpt, 'GOGPT'), (hydro, 'GHPT'), (wind,'GWPT'), (bio,'GBPT'),\n",
    "                        (solar, 'GSPT'), (geo, 'GGPT'), (nuclear, 'GNPT'), (goget_formatted, 'GOGET'), (ggit_lng, 'GGIT-lng'), (ggit, 'GGIT'), (goit, 'GOIT')]\n",
    "\n",
    "    file_name = f'/Users/gem-tah/Desktop/GEM_INFO/GEM_WORK/earthrise-maps/testing/output[misformmatted essential data from source]{new_release_date}.xlsx'\n",
    "    print(file_name)\n",
    "    with pd.ExcelWriter(file_name) as writer:\n",
    "        for tracker_tup in list_of_trackers:\n",
    "            df = tracker_tup[0]\n",
    "            tracker = tracker_tup[1]\n",
    "            print(tracker)\n",
    "            renaming_sel = renaming_cols_dict[tracker]\n",
    "            # print(renaming_sel)\n",
    "            df = df.rename(columns=renaming_sel) \n",
    "            \n",
    "            df['error messages'] = '' # initialize\n",
    "\n",
    "                \n",
    "            for col in misformatted_cols_check:\n",
    "                print(col)\n",
    "                if col in df.columns:\n",
    "                    ser = df[col].replace('', np.nan)\n",
    "                    try:\n",
    "                        ser_float = ser.astype(float)\n",
    "                    except:\n",
    "                        for row in ser.index:\n",
    "                            val = ser.at[row]\n",
    "                            try:\n",
    "                                val_float = float(val)\n",
    "                            except:\n",
    "                                # print(\"Error!\" + f\" val couldn't be converted to float: {val}\")\n",
    "                                df.at[row, 'error messages'] += \"Error!\" + f\" In col {col}, val couldn't be converted to float: '{val}'; \"\n",
    "\n",
    "    df = df[df['error messages']!='']\n",
    "    df['error messages'] = df['error messages'].str.strip('; ')\n",
    "    df.to_excel(writer, sheet_name='error messages', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "82fef1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing_source == True:\n",
    "\n",
    "    # RUN ON SOURCE DATA ONLY\n",
    "    # add a test to compare region and country list inconsistency\n",
    "\n",
    "    # find source data where key info is missing\n",
    "    # status\n",
    "    # geometry or lat lng\n",
    "    # include countries that are multiple so not in latam \n",
    "    missing_cols_check = ['geometry', 'Latitude', 'Longitude'] #'status', 'url', 'name', 'id', 'areas', \n",
    "    missing = {}\n",
    "    misformatted = {}\n",
    "    inconsistent = {} # create a check to compare region to country \n",
    "\n",
    "    list_of_trackers = [(coal_terminals, 'GCTT Coal Terminals'), (coal_mines, 'GCMT'), (gcpt,'GCPT'), (gogpt, 'GOGPT'), (hydro, 'GHPT'), (wind,'GWPT'), (bio,'GBPT'),\n",
    "                        (solar, 'GSPT'), (geo, 'GGPT'), (nuclear, 'GNPT'), (goget_formatted, 'GOGET'), (ggit_lng, 'GGIT-lng'), (ggit, 'GGIT'), (goit, 'GOIT')]\n",
    "    # list_of_trackers = [(coal_terminals, 'GCTT Coal Terminals'), (coal_mines, 'coal_mines'), (gcpt,'gcpt')]\n",
    "    file_name = f'/Users/gem-tah/Desktop/GEM_INFO/GEM_WORK/earthrise-maps/testing/output-missing-misformatted-essential-map-{new_release_date}{iso_today_date}.xlsx' # trackers_to_update\n",
    "\n",
    "    def check_missing_misformatted(df):\n",
    "        with pd.ExcelWriter(file_name, engine='openpyxl') as writer: \n",
    "\n",
    "            for tracker_tup in list_of_trackers:\n",
    "                df = tracker_tup[0]\n",
    "                tracker = tracker_tup[1]\n",
    "                renaming_sel = renaming_cols_dict[tracker]\n",
    "                df = df.rename(columns=renaming_sel) \n",
    "                df['error messages'] = '' # initialize\n",
    "                \n",
    "    # missing test\n",
    "\n",
    "                for col in missing_cols_check:\n",
    "                    df['error messages'] = '' # reset for each column so error messages relevant in tab\n",
    "\n",
    "                    print(col)\n",
    "                    if col in df.columns:\n",
    "                        print(f\"Data type of column '{col}': {df[col].dtype}\")\n",
    "                        print(f\"Number of missing values in column '{col}': {df[col].isna().sum()}\")\n",
    "                        df_missing_na = df[col].isna()\n",
    "                        # print(len(df))\n",
    "                        df_missing_nan = df.loc[df_missing_na]\n",
    "                        # print(len(df_missing_nan))\n",
    "                        df_missing_blank = df[df[col]=='']\n",
    "                        df_missing = pd.concat([df_missing_nan, df_missing_blank], axis=0)\n",
    "                        df_missing = df_missing.drop_duplicates(subset=['id'])\n",
    "                        if len(df_missing) > 0:\n",
    "                            missing[f'{tracker}-{col}'] = df_missing\n",
    "                            df_missing.to_excel(writer, sheet_name=f'{tracker}-{col}-missing', index=False)\n",
    "    # misformatted test\n",
    "                        ser = df[col].replace('', np.nan)\n",
    "                        try:\n",
    "                            ser_float = ser.astype(float)\n",
    "                        except:\n",
    "                            for row in ser.index:\n",
    "                                val = ser.at[row]\n",
    "                                try:\n",
    "                                    val_float = float(val)\n",
    "                                except:\n",
    "                                    df.at[row, 'error messages'] += \"Error!\" + f\" In col {col}, val couldn't be converted to float: '{val}'; \"\n",
    "                        \n",
    "                        \n",
    "                        df_misformatted = df[df['error messages']!=''] # means there is a col with a misformatted row \n",
    "                        df_misformatted['error messages'] = df_misformatted['error messages'].str.strip('; ')\n",
    "                        if len(df_misformatted) > 0:\n",
    "                            df_misformatted.to_excel(writer, sheet_name=f'{tracker}-{col}-misformatted', index=False)            \n",
    "                    else:\n",
    "                        print(f'{col} not in {tracker}')\n",
    "                # print(missing)\n",
    "\n",
    "    check_missing_misformatted(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9568a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tracker in set(final_map['tracker acro'].to_list()):\n",
    "    print(tracker)\n",
    "    df = final_map[final_map['tracker acro']==tracker]\n",
    "    print(len(df))\n",
    "    print(df['status'].value_counts())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7668176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check goit, ggit, ggitlng, goget\n",
    "# discrepancy = [(goit,'GOIT'), (ggit, 'GGIT'), (goget,'GOGET'), (ggit_lng,'GGIT-lng')]\n",
    "list_of_trackers = [(coal_terminals, 'GCTT'), (coal_mines, 'GCMT'), (gcpt,'GCPT'), (gogpt, 'GOGPT'), (hydro, 'GHPT'), (wind,'GWPT'), (bio,'GBPT'),\n",
    "                    (solar, 'GSPT'), (geo, 'GGPT'), (nuclear, 'GNPT'), (goget_formatted, 'GOGET'), (ggit_lng, 'GGIT-lng'), (ggit, 'GGIT'), (goit, 'GOIT')]\n",
    "\n",
    "list_of_trackers_dd = [(coal_terminals, 'GCTT Coal Terminals'), (coal_mines, 'GCMT Coal Mines'), (gcpt,'GCPT Coal Plants'), (gogpt, 'GOGPT Oil & Gas Plants'), (hydro, 'GHPT Hydropower'), (wind,'GWPT Wind'), (bio,'GBPT Bioenergy'),\n",
    "                    (solar, 'GSPT Solar'), (geo, 'GGPT Geothermal'), (nuclear, 'GNPT Nuclear'), (goget_formatted, 'GOGET Oil & Gas Extraction'), (ggit_lng, 'GGIT-lng LNG Terminals'), \n",
    "                    (ggit, 'GGIT Gas Pipelines'), (goit, 'GOIT Oil Pipelines')]\n",
    "\n",
    "# file_name = f'/Users/gem-tah/Desktop/GEM_INFO/GEM_WORK/earthrise-maps/testing/output_africa_discrepancies_{new_release_date}.xlsx'\n",
    "# print(file_name)\n",
    "# with pd.ExcelWriter(file_name) as writer:\n",
    "discrepancies = {}\n",
    "for source_tracker_tup in list_of_trackers:\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "    print(source_tracker_tup[1])\n",
    "    tracker = source_tracker_tup[1]\n",
    "    df = source_tracker_tup[0]\n",
    "    # filter by countries not region so can for goget\n",
    "    # africa_df = \n",
    "\n",
    "    renaming_sel = renaming_cols_dict[tracker]\n",
    "    df = df.rename(columns=renaming_sel) \n",
    "    map_df = final_map[final_map['tracker acro']==tracker]\n",
    "    if tracker == 'GOGET':\n",
    "        df = df[df['areas'].isin(Africa_countries)] # TODO fix for this because it excludes when there is multiple countries and shouldn't OG0013150 for africa\n",
    "        print(f'Source Data {tracker}: {len(df)}')\n",
    "        print(f'Map Data {tracker}: {len(map_df)}')\n",
    "        df_missing = df[~df['id'].isin(map_df['id'])]\n",
    "        df_missing_reverse = map_df[~map_df['id'].isin(df['id'])]\n",
    "        \n",
    "        print('Map Discrepancy Count')\n",
    "        print(len(df_missing))\n",
    "        print(df_missing_reverse)\n",
    "        if len(df_missing) > 0:\n",
    "            # print(df_missing)\n",
    "            discrepancies[tracker] = df_missing\n",
    "            \n",
    "    elif tracker == 'GCMT':\n",
    "        df = df[df['areas'].isin(Africa_countries)] # TODO fix for this because it excludes when there is multiple countries and shouldn't OG0013150 for africa\n",
    "        print(f'Source Data {tracker}: {len(df)}')\n",
    "        print(f'Map Data {tracker}: {len(map_df)}')\n",
    "        df_missing = df[~df['id'].isin(map_df['id'])]\n",
    "        df_missing_reverse = map_df[~map_df['id'].isin(df['id'])]\n",
    "        \n",
    "        print('Map Discrepancy Count')\n",
    "        print(len(df_missing))\n",
    "        print(df_missing_reverse)\n",
    "        if len(df_missing) > 0:\n",
    "            print(df_missing) \n",
    "            print(f'This is region for missing: {df_missing[\"region\"]}')\n",
    "        \n",
    "        # filter for no status\n",
    "        print('mapdf with no status gcmt:')\n",
    "        map_df['status'] = map_df['status'].fillna('')\n",
    "        map_df_no = map_df[map_df['status']=='']\n",
    "        print(map_df_no)\n",
    "        print('source df with no status gcmt:')   \n",
    "        df['status'] = df['status'].fillna('')\n",
    "        df_no = df[df['status']==''] \n",
    "        print(df_no)   \n",
    "    else:\n",
    "        df = df[df['region']=='Africa']\n",
    "        print(f'Source Data {tracker}: {len(df)}')\n",
    "        print(f'Map Data {tracker}: {len(map_df)}')\n",
    "\n",
    "        df_missing = df[~df['id'].isin(map_df['id'])]\n",
    "        df_missing_reverse = map_df[~map_df['id'].isin(df['id'])]\n",
    "\n",
    "        if len(df_missing) > 0:\n",
    "            print(f'df missing {df_missing}')\n",
    "            if 'geometry' in df_missing.columns:\n",
    "                df_investigation_7 = df_missing[df_missing['geometry']!= '']\n",
    "                # print(df_investigation_7['geometry'])\n",
    "            discrepancies[tracker] = df_missing\n",
    "        print('Map Discrepancy Count')\n",
    "        print(len(df_missing))\n",
    "        # if len(df_missing_reverse) >0:\n",
    "            # print(f'reverse {df_missing_reverse}')\n",
    "\n",
    "        \n",
    "for source_tracker_tup in list_of_trackers_dd:\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "    tracker_acro = source_tracker_tup[1].split(' ')[0]\n",
    "    tracker_official = \" \".join(source_tracker_tup[1].split(' ')[1:])\n",
    "    print(tracker_acro)\n",
    "\n",
    "    print(tracker_official)\n",
    "    df = source_tracker_tup[0]\n",
    "\n",
    "    renaming_sel = renaming_cols_dict[tracker_acro]\n",
    "    df = df.rename(columns=renaming_sel) \n",
    "    dd_df = final_data_dict[tracker_official].rename(columns=renaming_sel) \n",
    "    # renmae dd_df too\n",
    "    # print(dd_df.columns)\n",
    "    if tracker_acro == 'GOGET':\n",
    "        df = df[df['areas'].isin(Africa_countries)]\n",
    "        df_missing_dd = df[~df['id'].isin(dd_df['id'])]\n",
    "        df_missing_dd_reverse = dd_df[~dd_df['id'].isin(df['id'])]\n",
    "\n",
    "        print(f'Source Data {tracker_acro}: {len(df)}')\n",
    "        print(f'Data Download Data {tracker_acro}: {len(dd_df)}')\n",
    "        print('DD Discrepancy Count')\n",
    "        print(len(df_missing_dd))\n",
    "        if len(df_missing_dd) > 0:\n",
    "            # print(df_missing_dd['region'])\n",
    "            discrepancies[tracker_acro] = df_missing_dd\n",
    "\n",
    "    else:\n",
    "        df = df[df['region']=='Africa']\n",
    "        print(f'Source Data {tracker_acro}: {len(df)}')\n",
    "        print(f'Data Download Data {tracker_acro}: {len(dd_df)}')\n",
    "        df_missing_dd = df[~df['id'].isin(dd_df['id'])]\n",
    "        df_missing_dd_reverse = dd_df[~dd_df['id'].isin(df['id'])]\n",
    "\n",
    "        print('DD Discrepancy Count')\n",
    "        print(len(df_missing_dd))\n",
    "        if len(df_missing_dd) > 0:\n",
    "            # print(df_missing_dd['region'])\n",
    "            discrepancies[tracker_acro] = df_missing_dd\n",
    "        # print(df_missing_dd_reverse)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
